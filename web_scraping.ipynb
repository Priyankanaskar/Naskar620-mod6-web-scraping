{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P6: Web Scraping, NLP (Requests, BeautifulSoup, and spaCy) & Engage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project - Web Scraping and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "This exercise is used to practice web scraping (fetching and extracting information) and processing the content from a web page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : Priyanka Naskar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub link: https://github.com/Priyankanaskar/Naskar620-mod6-web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Content from the Web\n",
    "\n",
    "## When you have no API... That's annoying!\n",
    "\n",
    "Sometimes we need to try to access data on a webpage that does not have a helpful API that bundles and returns our data in a usable manner.  When that happens, we are left with no choice but to resort to web scraping, that is downloading the actual webpage (or parts of the webpage) and manipulating the text or elements of the HTML to obtain the information we want.\n",
    "\n",
    "## Tools to install:\n",
    "\n",
    "Use either `conda` or `pip` to install the following packages (e.g. `pip install beautifulsoup4`) depending on your environment:\n",
    "\n",
    "* `beautifulsoup4`\n",
    "* `html5lib` (optional parser - can use the built-in 'html.parser' instead)\n",
    "\n",
    "We will deal with each of these tools in turn.  BS4 is a package that allows you to fully read and do horrible horrible things to the DOM of a webpage (it's an XML parser that builds a parse tree for you to play with).  Boilerpipe is a python wrapper to a popular java library that can streamline some of the process of extracting parts of a webpage.  Feedparser lets you get RSS and Atom feeds (remember those?)\n",
    "\n",
    "Many examples are motivated from the content at https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition/blob/master/notebooks/Chapter%206%20-%20Mining%20Web%20Pages.ipynb\n",
    "\n",
    "## Getting page text with `requests` and `BeautifulSoup4`\n",
    "\n",
    "A majority of the web does not follow a specific standard (RSS/ATOM Feeds, etc), or the data we want is in a particularly odd format.  For this we need more flexible tools to allow us to obtain a full web page and extract meaning from it.  For this we will use the `requests` module and `beautifulsoup` to pull our information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacytextblob in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (5.0.0)\n",
      "Requirement already satisfied: spacy>=3.0.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacytextblob) (3.8.2)\n",
      "Requirement already satisfied: textblob>=0.18.0.post0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacytextblob) (0.18.0.post0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->spacytextblob) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->spacytextblob) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->spacytextblob) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->spacytextblob) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->spacytextblob) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=3.0.0->spacytextblob) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from spacy>=3.0.0->spacytextblob) (2.0.2)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob>=0.18.0.post0->spacytextblob) (3.9.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->spacytextblob) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob>=0.18.0.post0->spacytextblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob>=0.18.0.post0->spacytextblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob>=0.18.0.post0->spacytextblob) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->spacytextblob) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->spacytextblob) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.0->spacytextblob) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacytextblob) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacytextblob) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacytextblob) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacytextblob) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.0.0->spacytextblob) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.0.0->spacytextblob) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=3.0.0->spacytextblob) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->spacytextblob) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.0->spacytextblob) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->spacytextblob) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.0->spacytextblob) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy>=3.0.0->spacytextblob) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\priya\\appdata\\roaming\\python\\python312\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.0->spacytextblob) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->spacytextblob) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->spacytextblob) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.0->spacytextblob) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall spacytextblob\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Import the libraries for use\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacytextblob\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspacytextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpacyTextBlob\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Only need to run these once in the Jupyter notebook (to ensure packages are installed in the notebook environment)\n",
    "%pip install spacy\n",
    "%pip install spacytextblob\n",
    "\n",
    "# Import the libraries for use\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray... raw HTML.  This will clearly be a good day.  Let's... find a way to **not** deal with raw html encoded text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# parser = 'html5lib'\n",
    "parser = 'html.parser'\n",
    "\n",
    "soup = BeautifulSoup(response.text, parser)\n",
    "# Uncomment next lines to explore full page contents; it's long so when done, recomment\n",
    "# print(soup)\n",
    "# print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our soup we can extract information by finding tags, searching for ids, and in general treating the parse tree of the HTML as a parse tree.  `BeautifulSoup4` allows us to search for elements of our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 header: <h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\"><span class=\"mw-page-title-main\">Data mining</span></h1>\n",
      "h1 text: Data mining\n"
     ]
    }
   ],
   "source": [
    "for header in soup.findAll('h1'):\n",
    "    print('h1 header:', header)\n",
    "    print('h1 text:', header.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some workable material\n",
    "\n",
    "Let's start with an article and find learn some information about it.  Our first step should be to obtain our web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_page = requests.get('http://web.archive.org/web/20210415020310/https://hackaday.com/2021/04/02/python-will-soon-support-switch-statements/')\n",
    "article_html = article_page.text\n",
    "\n",
    "# pickle works similar to json, but stores information in a binary format\n",
    "# json files are readable by humans, pickle files, not so much\n",
    "\n",
    "# BeautifulSoup objects don't pickle well, so it's appropriate and polite to web developers to cache the text of the web page, or just dump it to an html file you can read in later as a regular file\n",
    "import pickle\n",
    "with open('python-match.pkl', 'wb') as f:\n",
    "    pickle.dump(article_page.text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('python-match.pkl', 'rb') as f:\n",
    "    article_html = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to make our HTML page searchable/usable.  Parsing HTML by hand would be a waste of time, so let's ask BeautifulSoup to make us some delicious soup.\n",
    "\n",
    "Use either parser - how can you find the pros and cons of 'html5lib' or the built-in 'html.parser'? \n",
    "\n",
    "Where did we set the value of the parser variable used below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(article_html, parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we want to extract the article text from the page.  Luckily, we have the ability to investigate and inspect individual elements.  By using the inspector we can find that the article is contained in an `article` element; how convenient!  This might not be the case for every page.  One problem with web scraping is that you need to specialize your code for whatever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_element = soup.find('article')\n",
    "# Uncomment to see the entire article element html; again, it's long\n",
    "# print(article_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the article_content, you see that we get the html contained in the article element; while this is the content we want there's a lot of HTML cruft in there that won't help us (we are interested in text, probably).  Luckily for us, BeautifulSoup allows us to essentially display the text (roughly) as a web browser would display it using the `get_text()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Python Will Soon Support Switch Statements\n",
      "\n",
      "\n",
      "                112 Comments            \n",
      "\n",
      "by:\n",
      "Adam Zeloof\n",
      "\n",
      "\n",
      "\n",
      "April 2, 2021\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rejoice! Gone are the long chains of if…else statements, because switch statements will soon be here — sort of. What the Python gods are actually giving us are match statements. match statements are awfully similar to switch statements, but have a few really cool and unique features, which I’ll attempt to illustrate below.\n",
      "\n",
      "Flip The Switch\n",
      "A switch statement is often used in place of an if…else ladder. Here’s a quick example of the same logic in C, first executed with an if statement, and then with a switch statement:\n",
      "\n",
      "Essentially, a switch statement takes a variable and tests it for equality against a number of different cases. If none of the cases match, then the default case is invoked. Notice in the example that each case is terminated by a break. This protects against more than one case matching (or allows for cascading), as the cases are checked in the order in which they are listed. Once a case is completed, break is called and the switch statement exits.\n",
      "A Match Made In Heaven\n",
      "\n",
      "You can think of match statements as “Switch 2.0”. Before we get into the nitty-gritty here, if all you want is switch in Python then you’re in luck, because they can be used in the same way. Here’s a similar example to what we looked at earlier, this time using match in Python.\n",
      "There are a few differences to note right off the bat. First, there are no break statements. The developers were concerned about confusion if a break statement were called inside a match statement inside a loop — what breaks then, the loop or the match? Here, the first case that is satisfied is executed, and the statement returns. You’ll also notice that rather than default, we have case _, which behaves in the same way.\n",
      "The Power of Pattern Matching\n",
      "So, we’ve got a switch statement with slightly different syntax, right? Not quite. The name match was used for a reason — what’s actually going on here is something called Pattern Matching. To illustrate what that is, let’s look at a more exciting example, right out of the feature proposal to add the keyword in question to Python:\n",
      "\n",
      "Wow! We just took an object, checked its type, checked it’s shape, and instantiated a new object without any indexing, or even a len() call. It also works on any type, unlike the classic switch which only works on integral values. In case this wasn’t cool enough for you, patterns can be combined. Again, from the feature proposal:\n",
      "\n",
      "Okay, okay — one more example. This is where the match statement gets really powerful. Patterns themselves can include comparisons, known as guards. This lets you filter values within each case statement:\n",
      "\n",
      "Sold! When Can I Try?\n",
      "We’ll get our hands on this magical new command in Python 3.10, slated for a full release on October 4th, 2021. For the adventurous, an alpha version (Python 3.10.0a6) is available for download today. It might be worth spending some time getting acquainted with the pattern matching, like understanding the difference between matching literal values and matching variables.\n",
      "So why doesn’t every language have match statements? They’re clearly better than switch statements!\n",
      "That’s what I said at least, and my girlfriend Sara was quick to raise her eyebrows and explain that there’s a huge performance overhead involved. The switch statement in C is relatively simple. It compares integers to one another, executing n constant-time operations. All of the power and convenience that comes with the match statement means a lot is going on in the background, which in turn slows down the code execution — an incredibly Pythonic tradeoff to make.\n",
      "I find an efficiency hit a small price to pay for such expanded functionality, but as a Mechanical Engineer my favorite languages are Matlab and Python so you probably should take my opinion here with a grain of salt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posted in Featured, News, Slider, Software DevelopmentTagged match, python, switch \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(article_element.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have raw article text! Let the NLP begin! (also introducing/reintroducing [f strings](https://docs.python.org/3/tutorial/inputoutput.html#tut-f-strings)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacytextblob\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspacytextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpacyTextBlob\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# why not, let's add some fun sentiment analysis, because we can\n",
    "nlp.add_pipe('spacytextblob')\n",
    "doc = nlp(article_element.get_text())\n",
    "print(f'Polarity: {doc._.polarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So does the article have an overall positive overtone? What polarity scores indicate positive tone? What scores indicate negative? Is -0.5 more negative than -0.2?\n",
    "\n",
    "I'm actually pretty excited about match statements in Python, but I'm a nerd, so... you know.  What else can we do?\n",
    "\n",
    "## NLP: Finding important terms and introducing \"stopwords\"\n",
    "\n",
    "Our `doc` object contains information about every single lexeme in the text we had it parse.  Let's look at what these are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lexeme in doc[:10]: # just the first 10 for now\n",
    "    print('---',lexeme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the large blocks of whitespace are considered tokens; that's not something we particularly care about.  Let's start by going through our document and only getting tokens that aren't whitespace (I'll do this one as a loop constructing the list).  Notice; each lexeme in the document has a flag for whether it is whitespace or not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ws_tokens = []\n",
    "for token in doc:\n",
    "    if not token.is_space:\n",
    "        non_ws_tokens.append(token)\n",
    "print(non_ws_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that punctuation is also its own token, so let's get rid of it.  At this point it starts to make sense to define a function to determine if we care about a particular token.  Let's build our list with a comprehension this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def we_care_about(token):\n",
    "    return not (token.is_space or token.is_punct)\n",
    "\n",
    "interesting_tokens = [token for token in doc if we_care_about(token)]\n",
    "print(interesting_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the most frequent terms; that's an interesting way to determine the most important parts of a body of text.  Note the use of the Counter collection data structure; it's essentially a superpowered dictionary.  Note that because tokens have context, they won't be the same objects (everything will have a frequency of 1) so we turn all of them into strings using the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_freq = Counter(map(str,interesting_tokens))\n",
    "print(word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hrm; most of the top 10 most common words are not particularly interesting.  The words \"the\", \"a\", etc. do little to add to the meaning of a sentence but they do help with human comprehension.  We call such words \"stopwords\".  Let's modify our code so we exclude stopwords in our count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def we_care_about(token):\n",
    "    return not (token.is_space or token.is_punct or token.is_stop)\n",
    "\n",
    "interesting_tokens = [token for token in doc if we_care_about(token)]\n",
    "word_freq = Counter(map(str,interesting_tokens))\n",
    "print(word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we suddenly see that the most common terms in an article about Python supporting switch statements now contains the terms \"statement\", \"statements\", \"Python\", \"Switch\", and \"match\".  You might notice that there are a few issues with this.\n",
    "\n",
    "* \"Switch\" and \"switch\" are both included; we could fix this by converting every string to lower\n",
    "* \"statement\" and \"statements\", as well as \"match\" and \"matching\" are counted as different terms, even though they have the same base.\n",
    "\n",
    "The second problem is harder to solve, or it would be if we weren't using such a powerful library. As you learned in your initial exploration of spaCy, part of the pipeline is Lemmatization.  This is the process of finding the base form of each word.  Let's see what happens when we use the base form of each word instead of the token itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_lemmas = [token.lemma_ for token in doc if we_care_about(token)]\n",
    "lemma_freq = Counter(interesting_lemmas)\n",
    "print(lemma_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still run into case sensitivity, but we can fix that by converting the string to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_lemmas = [token.lemma_.lower() for token in doc if we_care_about(token)]\n",
    "lemma_freq = Counter(interesting_lemmas)\n",
    "print(lemma_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!  Let's store the 5 most common words in a set and try some experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_words = set()\n",
    "for lemma, freq in lemma_freq.most_common(5):\n",
    "    cool_words.add(lemma)\n",
    "print(cool_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an experiment, let's see how many words in each sentence are important (or \"cool\" as I've named the set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents) # Thanks spaCy for just giving us our sentences\n",
    "for sentence in sentences:\n",
    "    count = 0\n",
    "    for token in sentence:\n",
    "        if token.lemma_.lower() in cool_words:\n",
    "            count += 1\n",
    "    # because there's a bunch of junk newlines, we'll replace those with nothing, as well as a little bit of whitespace\n",
    "    sent_str = str(sentence).replace('\\n','').replace('  ',' ')\n",
    "    print(count,':', sent_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: How many words in a sentence?\n",
    "\n",
    "When counting words, it's probably best to avoid whitespace or punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length (sent):\n",
    "    count = 0\n",
    "    for token in sent:\n",
    "        if not(token.is_space or token.is_punct):\n",
    "            count += 1\n",
    "    return count\n",
    "print(sentence_length(sentences[0]), sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ ------------\n",
      "annotated-types    0.7.0\n",
      "asttokens          2.4.1\n",
      "blis               1.0.1\n",
      "catalogue          2.0.10\n",
      "certifi            2024.8.30\n",
      "charset-normalizer 3.4.0\n",
      "click              8.1.7\n",
      "cloudpathlib       0.20.0\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "confection         0.1.5\n",
      "cymem              2.0.8\n",
      "debugpy            1.6.7\n",
      "decorator          5.1.1\n",
      "exceptiongroup     1.2.2\n",
      "executing          2.1.0\n",
      "filelock           3.16.1\n",
      "fsspec             2024.10.0\n",
      "huggingface-hub    0.26.2\n",
      "idna               3.10\n",
      "importlib_metadata 8.5.0\n",
      "ipykernel          6.29.5\n",
      "ipython            8.29.0\n",
      "jedi               0.19.2\n",
      "Jinja2             3.1.4\n",
      "joblib             1.4.2\n",
      "jupyter_client     8.6.3\n",
      "jupyter_core       5.7.2\n",
      "langcodes          3.4.1\n",
      "language_data      1.2.0\n",
      "marisa-trie        1.2.1\n",
      "markdown-it-py     3.0.0\n",
      "MarkupSafe         3.0.2\n",
      "matplotlib-inline  0.1.7\n",
      "mdurl              0.1.2\n",
      "mpmath             1.3.0\n",
      "murmurhash         1.0.10\n",
      "nest_asyncio       1.6.0\n",
      "networkx           3.4.2\n",
      "nltk               3.9.1\n",
      "numpy              2.0.2\n",
      "packaging          24.2\n",
      "parso              0.8.4\n",
      "pickleshare        0.7.5\n",
      "pip                24.3.1\n",
      "platformdirs       4.3.6\n",
      "preshed            3.0.9\n",
      "prompt_toolkit     3.0.48\n",
      "psutil             5.9.0\n",
      "pure_eval          0.2.3\n",
      "pydantic           2.9.2\n",
      "pydantic_core      2.23.4\n",
      "Pygments           2.18.0\n",
      "python-dateutil    2.9.0\n",
      "pywin32            305.1\n",
      "PyYAML             6.0.2\n",
      "pyzmq              25.1.2\n",
      "regex              2024.11.6\n",
      "requests           2.32.3\n",
      "rich               13.9.4\n",
      "safetensors        0.4.5\n",
      "setuptools         75.1.0\n",
      "shellingham        1.5.4\n",
      "six                1.16.0\n",
      "smart-open         7.0.5\n",
      "spacy              3.8.2\n",
      "spacy-legacy       3.0.12\n",
      "spacy-loggers      1.0.5\n",
      "spacytextblob      5.0.0\n",
      "srsly              2.4.8\n",
      "stack-data         0.6.2\n",
      "sympy              1.13.1\n",
      "textblob           0.18.0.post0\n",
      "thinc              8.3.2\n",
      "tokenizers         0.20.3\n",
      "torch              2.5.1\n",
      "tornado            6.4.1\n",
      "tqdm               4.67.0\n",
      "traitlets          5.14.3\n",
      "transformers       4.46.2\n",
      "typer              0.13.0\n",
      "typing_extensions  4.12.2\n",
      "urllib3            2.2.3\n",
      "vaderSentiment     3.3.2\n",
      "wasabi             1.1.3\n",
      "wcwidth            0.2.13\n",
      "weasel             0.4.1\n",
      "wheel              0.44.0\n",
      "wrapt              1.16.0\n",
      "zipp               3.21.0\n",
      "All prereqs installed.\n"
     ]
    }
   ],
   "source": [
    "# Create and activate a Python virtual environment. \n",
    "# Before starting the project, try all these imports FIRST\n",
    "# Address any errors you get running this code cell \n",
    "# by installing the necessary packages into your active Python environment.\n",
    "# Try to resolve issues using your materials and the web.\n",
    "# If that doesn't work, ask for help in the discussion forums.\n",
    "# You can't complete the exercises until you import these - start early! \n",
    "# We also import pickle and Counter (included in the Python Standard Library).\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "\n",
    "!pip list\n",
    "\n",
    "print('All prereqs installed.')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
